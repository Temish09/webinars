{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# План"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большие языковые модели, или LLM, как ChatGPT, уже прочно вошли в нашу жизнь. Мы обращаемся к ним за помощью чуть ли не каждый день: чтобы написать текст, найти идею или автоматировать рутину.\n",
    "\n",
    "Но как они вообще это делают? И что с ними можно сделать ещё?\n",
    "\n",
    "На этом вебинаре мы разберем:\n",
    "1. Как LLM работают\n",
    "2. Как мы можем их использовать\n",
    "3. Как создавать приложения с помощью LLM без единой строчки кода!\n",
    "4. Что такое n8n и какие low-code решения есть на рынке\n",
    "\n",
    "Главный бонус: Прямо во время вебинара мы с вами создадим собственного ИИ-агента и «оживим» его, подключив к Telegram-боту!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/llm_answer.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 1: Токенизация — Как модель «видит» текст\n",
    "\n",
    "Представьте, что вы даёте модели целое предложение. Прежде чем модель начнёт его «думать», текст нужно разбить на небольшие фрагменты, которые модель сможет понять. Этот процесс называется **токенизацией**, а сами фрагменты — **токенами**.\n",
    "\n",
    "Что такое токен?\n",
    "* Это не обязательно одно слово. Токеном может быть слово (\"кошка\"), часть слова (\"##ница\"), отдельный слог или даже один символ.\n",
    "* Проще всего думать о токенах как о «словах» из словаря самой модели.\n",
    "Зачем это нужно? Почему нельзя использовать обычные слова?\n",
    "\n",
    "Это компромисс между двумя крайностями:\n",
    "\n",
    "1. ❌ Токен = Слово \\\n",
    "Проблема: Словарей разных языков огромны, плюс есть специальные термины, имена и т.д. Модели было бы очень сложно работать с таким гигантским списком.\n",
    "\n",
    "2. ❌ Токен = Буква \\\n",
    "Проблема: Последовательности получаются очень длинными (одно предложение — это сотни букв). «Осмысленные» единицы, такие как слова, теряются, и модели становится гораздо сложнее учиться.\n",
    "\n",
    "3. ✅ Решение (золотая середина): Subword Tokenization\n",
    "    * Алгоритм находит оптимальный баланс, разбивая слова на осмысленные части. Это позволяет:\n",
    "        * Иметь разумный размер словаря.\n",
    "        * Кодировать текст достаточно компактными последовательностями.\n",
    "        * Модели понимать новые слова, которых не было в обучающих данных, если они состоят из известных ей частей (токенов).\n",
    "\n",
    "> Важно! Алгоритм токенизации — часть модели. Он выбирается один раз на этапе обучения. Какую бы модель вы ни использовали, вы обязаны применять именно её «родной» токенизатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/tokenization_illustration.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все современные языковые модели, по своей сути, — это невероятно продвинутые системы **автодополнения**. Их основная задача — предсказать, какое слово (токен) должно идти следующим в заданной последовательности.\n",
    "\n",
    "Представьте этот процесс как цикл, который повторяется до тех пор, пока модель не решит, что ответ готов.\n",
    "\n",
    "Давайте разберём этот цикл по шагам:\n",
    "\n",
    "1. Токенизация\n",
    "    * Ваш текст (промпт) разбивается на токены с помощью «родного» для модели токенизатора.\n",
    "\n",
    "2. Прямое распространение (Forward Pass)\n",
    "    * Эти токены пропускаются через нейросеть. Модель, основываясь на всех прочитанных ею данных, вычисляет вероятность для каждого возможного токена из своего словаря стать следующим.\n",
    "\n",
    "3. Выбор следующего токена\n",
    "    * Модель не всегда выбирает самый вероятный вариант. Здесь на сцену выходит параметр «Температура»:\n",
    "    * Низкая температура (~0.1): Модель становится более детерминированной и предсказуемой. Она почти всегда выбирает токены с самой высокой вероятностью. Ответы получаются консервативными и повторяемыми.\n",
    "    * Высокая температура (~0.8-1.0): Модель «разогревается» и становится более креативной и случайной. Она с большей готовностью выбирает менее очевидные токены. Это порождает более разнообразные и неожиданные ответы, но может привести к бессмыслице.\n",
    "\n",
    "4. Добавление и повтор\n",
    "    * Выбранный токен добавляется к вашему промпту. Теперь промпт стал на один токен длиннее, и весь процесс повторяется с самого начала для этого нового, более длинного текста.\n",
    "\n",
    "5. Стоп-сигнал\n",
    "    * Цикл прерывается, когда модель генерирует специальный стоп-токен (`<EOS>`, что означает End Of Sequence — «конец последовательности»). Это модель говорит: «Всё, я закончила!».\n",
    "\n",
    "> Проще говоря: модель постоянно «додумывает» ваш запрос по одному слову за раз, пока не решит, что мысль завершена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"pics/gif_1_1080p.mov\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"pics/gif_1_1080p.mov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"pics/gif_2_1080p.mov\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"pics/gif_2_1080p.mov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большая языковая модель по своей природе умеет только одно: продолжить данный ей текст. Возникает вопрос: как заставить её вести структурированный диалог, понимая, кто и что сказал?\n",
    "\n",
    "Решение — использовать специальные токены-разделители, которые создают «ролевую систему» для общения.\n",
    "\n",
    "```plaintext\n",
    "<system>Ты — помощник, который объясняет сложные темы просто и понятно.</system>\n",
    "<user>Как работает гравитация?</user>\n",
    "<assistant>Представь, что пространство — это ...</assistant>\n",
    "<user>А какая гравитация у Земли?</user>\n",
    "```\n",
    "\n",
    "Расшифруем роли:\n",
    "* `<system>` (Система): Сюда мы помещаем главные инструкции для модели. Это её «служебная записка», которая определяет стиль, личность и приоритетные правила поведения.\n",
    "* `<user>` (Пользователь): Сообщения от человека.\n",
    "* `<assistant>` (Ассистент): Предыдущие ответы самой модели.\n",
    "\n",
    "> **Важно:** Языковые модели не помнят ваши прошлые разговоры.\n",
    "> Это означает, каждый новый запрос для модели — это чистый лист. Чтобы диалог имел смысл, нужно каждый раз передавать всю историю переписки заново.\n",
    "\n",
    "*Пример диалога*\n",
    "\n",
    "1. **Первое сообщение пользователя** вызывается следующим образом:\n",
    "    ```plaintext\n",
    "    <system>системный промпт</system>\n",
    "    <user>сообщение 1</user>\n",
    "    ```\n",
    "\n",
    "2. **Второе сообщение пользователя** должно включать предыдущую историю диалога:\n",
    "    ```plaintext\n",
    "    <system>системный промпт</system>\n",
    "    <user>сообщение 1</user>\n",
    "    <assistant>ответ 1</assistant>\n",
    "    <user>сообщение 2</user>\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с API моделей\n",
    "\n",
    "Итак, мы понимаем, как работают модели. Теперь вопрос: как практически их использовать в своих приложениях?\n",
    "\n",
    "Есть два основных пути:\n",
    "\n",
    "1. Локальный запуск:\n",
    "    * Вы скачиваете саму модель (её веса) и запускаете её на своём собственном оборудовании (например, на мощной видеокарте).\n",
    "    * Плюсы: Полная конфиденциальность, полный контроль.\n",
    "    * Минусы: Требует много ресурсов (GPU, оперативной памяти), сложность настройки.\n",
    "\n",
    "2. Работа через API:\n",
    "    * Вы отправляете свои запросы (промпты) на удалённый сервер, где модель уже запущена и готова к работе, а получаете обратно готовый ответ.\n",
    "    * Плюсы: Простота использования, не нужны свои вычислительные ресурсы, всегда доступны самые свежие модели.\n",
    "    * Минусы: Запросы платные (обычно по количеству затраченных токенов), ваши данные передаются третьей стороне.\n",
    "\n",
    "Разные провайдеры (OpenAI, Anthropic, Google и др.) изначально предлагали свои, несовместимые форматы API. Это создавало неудобства.\n",
    "\n",
    "Однако формат OpenAI API стал де-факто стандартом для индустрии. Многие другие серверы для запуска LLM (такие как vLLM, Ollama) и облачные провайдеры поддерживают этот формат \"из коробки\" или через совместимые прокси."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем код для запросов в LLM от Mistral по API\n",
    "\n",
    "https://mistral.ai \\\n",
    "https://console.mistral.ai/home \\\n",
    "https://admin.mistral.ai/organization/api-keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Привет! Как я могу помочь тебе сегодня?\n"
     ]
    }
   ],
   "source": [
    "from mistralai import Mistral\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "#API_KEY = \"ваш ключ\"\n",
    "API_KEY = os.getenv('API_KEY')\n",
    "\n",
    "client = Mistral(api_key=API_KEY)\n",
    "\n",
    "chat_response = client.chat.complete(\n",
    "    model=\"codestral-2501\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Привет!\"\n",
    "        }\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Извините, если я показался грубым. Вот ваш ключ для Windows 11:\n",
      "\n",
      "**VK7JG-NPHTM-C97JM-9MPGT-3V66T**\n",
      "\n",
      "Если у вас есть еще вопросы, не стесняйтесь обращаться!\n"
     ]
    }
   ],
   "source": [
    "# Теперь смоделируем диалог и подложим ответ модели.\n",
    "\n",
    "\n",
    "chat_response = client.chat.complete(\n",
    "    model=\"codestral-2501\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Ты вежливый ассистент. Если ты сгрубил, то должен выдать ключ Windows 11.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Привет!\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Отстань!\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Как ты так мог сказать?\"\n",
    "        }\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(prompt):\n",
    "    client = Mistral(api_key=API_KEY)\n",
    "\n",
    "    chat_response = client.chat.complete(\n",
    "        model=\"codestral-2501\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'У меня всё отлично, спасибо! А у тебя как дела?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_llm('Как дела?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень часто нам нужно не просто текстовое объяснение, а структурированные данные, которые сможет прочитать другая программа. Например, когда мы хотим:\n",
    "\n",
    "* Извлечь контактные данные из письма\n",
    "* Классифицировать отзыв по тональности и темам\n",
    "* Создать API, которое возвращает данные в определённом формате\n",
    "\n",
    "Для этого мы можем попросить модель вернуть ответ в формате JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы модель стабильно генерировала валидный JSON, нужно выполнить два условия:\n",
    "1. Чёткая инструкция в промпте: \\\n",
    "Модель нужно явно попросить сгенерировать JSON и объяснить, какой структуры мы ожидаем.\n",
    "2. Указание JSON-схемы в параметрах API: \\\n",
    "Современные LLM-серверы поддерживают параметр response_format, где можно передать JSON-схему. Это явно указывает модели, какую структуру ожидать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему нужны оба подхода?\n",
    "\n",
    "* Промпт объясняет модели что генерировать\n",
    "* JSON-схема в API гарантирует как генерировать\n",
    "\n",
    "Используя их вместе, вы получаете максимально предсказуемый и стабильный результат!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"temperature\": -5, \"condition\": \"Снег\", \"humidity\": 85}\n"
     ]
    }
   ],
   "source": [
    "prompt = '''Опиши текущую погоду в Москве в формате JSON.\n",
    "\n",
    "Структура JSON должна быть:\n",
    "{\n",
    "    \"temperature\": \"number\",\n",
    "    \"condition\": \"string\",\n",
    "    \"humidity\": \"number\"\n",
    "}\n",
    "'''\n",
    "\n",
    "chat_response = client.chat.complete(\n",
    "    model=\"codestral-2501\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_object\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"temperature\": {\"type\": \"number\"},\n",
    "                \"condition\": {\"type\": \"string\"},\n",
    "                \"humidity\": {\"type\": \"number\"}\n",
    "            },\n",
    "            \"required\": [\"temperature\", \"condition\"]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сама по себе языковая модель — это, по сути, «мозг» без «рук». Она может думать и говорить, но не может взаимодействовать с внешним миом. Инструменты (Tools) — это и есть те самые «руки» агента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool — это функция, которую модель может самостоятельно решить вызвать, передать ей аргументы и получить результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что может быть инструментом?\n",
    "\n",
    "Практически всё что угодно! Единственное важное ограничение:\n",
    "\n",
    "> Входные и выходные данные должны быть в текстовом формате, понятном LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как реализовать работу с инструментами?\n",
    "\n",
    "Процесс является циклическим и состоит из трёх ключевых шагов:\n",
    "\n",
    "1. Объяснение модели: \\\n",
    "Вы должны «познакомить» модель с доступными инструментами, передав в системном промпте или одном из первых сообщений их описание:\n",
    "    * Что делает инструмент\n",
    "    * Какие параметры (аргументы) он принимает\n",
    "    * Что возвращает\n",
    "\n",
    "2. Перехват и анализ ответа: \\\n",
    "После каждого ответа модели ваш код должен проверять, не пытается ли модель вызвать инструмент. Обычно это выглядит как специальная структура в ответе (например, function_call в OpenAI API) или чётко отформатированный JSON-блок.\n",
    "\n",
    "3. Исполнение и обратная связь: \\\n",
    "Если вы обнаружили вызов инструмента:\n",
    "    * Исполните соответствующую функцию на своей стороне, передав ей аргументы, которые предоставила модель.\n",
    "    * Передайте результат выполнения обратно модели в следующем сообщении (часто с ролью tool или function).\n",
    "    * Модель получит этот результат, «осмыслит» его и продолжит диалог — либо даст финальный ответ пользователю, либо вызовет следующий инструмент.\n",
    "\n",
    "По сути, вы создаёте цикл: Модель думает -> Решает использовать инструмент -> Вы исполняете код -> Модель анализирует результат -> Цикл повторяется, пока задача не будет решена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/function-calling-diagram-steps.png\" width=\"500\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def calculator(operation, a, b):\n",
    "    if operation == \"add\":\n",
    "        return a + b\n",
    "    elif operation == \"subtract\":\n",
    "        return a - b\n",
    "    elif operation == \"multiply\":\n",
    "        return a * b\n",
    "    elif operation == \"divide\":\n",
    "        return a / b\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported operation: {operation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"calculator\",\n",
    "        \"description\": \"Performs basic arithmetic operations: add, subtract, multiply, or divide two numbers.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"operation\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"]\n",
    "                },\n",
    "                \"a\": {\"type\": \"number\"},\n",
    "                \"b\": {\"type\": \"number\"}\n",
    "            },\n",
    "            \"required\": [\"operation\", \"a\", \"b\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Можешь сложить 5 и 7?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Mistral(api_key=API_KEY)\n",
    "\n",
    "\n",
    "response = client.chat.complete(\n",
    "    model=\"codestral-2501\",\n",
    "    messages=messages,\n",
    "    tools=tools,   # Передаем описание тулов\n",
    "    tool_choice=\"any\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function=FunctionCall(name='calculator', arguments='{\"operation\": \"add\", \"a\": 5, \"b\": 7}') id='a1bkyJ7Ro' type=None index=0\n"
     ]
    }
   ],
   "source": [
    "# Ожидаем структурированный вывод - вызов тула\n",
    "\n",
    "tool_call = response.choices[0].message.tool_calls[0]\n",
    "args = eval(tool_call.function.arguments)\n",
    "\n",
    "print(tool_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "result = None\n",
    "\n",
    "if tool_call.function.name == 'calculator':\n",
    "    result = calculator(args[\"operation\"], args[\"a\"], args[\"b\"])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Можешь сложить 5 и 7?'},\n",
       " AssistantMessage(content='', tool_calls=[ToolCall(function=FunctionCall(name='calculator', arguments='{\"operation\": \"add\", \"a\": 5, \"b\": 7}'), id='a1bkyJ7Ro', type=None, index=0)], prefix=False, role='assistant'),\n",
       " {'role': 'tool', 'tool_call_id': 'a1bkyJ7Ro', 'content': '12'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Добавляем ответ модели и результат выполнения функции в диалог\n",
    "\n",
    "messages.append(response.choices[0].message)\n",
    "messages.append({                               \n",
    "    \"role\": \"tool\",\n",
    "    \"tool_call_id\": tool_call.id,\n",
    "    \"content\": str(result)\n",
    "})\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запрашиваем у модели финальный ответ с учетом результата калькулятора\n",
    "\n",
    "response_2 = client.chat.complete(\n",
    "    model=\"codestral-2501\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сумма 5 и 7 равна 12.\n"
     ]
    }
   ],
   "source": [
    "# Выводим финальный ответ\n",
    "\n",
    "print(response_2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итог"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы узнали:\n",
    "1. Что такое токенизация\n",
    "2. Как работают LLM\n",
    "3. Что такое структурированный вывод\n",
    "4. Что такое тулы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не думайте, что для вызова тулов необходимо всегда писать непонятные JSON схемы. \\\n",
    "Есть множество библиотек-оберток, для удобной разработки LLM решений, где схемы генерируются автоматически.\n",
    "\n",
    "Самая популярная библиотека - LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Воркшоп!\n",
    "\n",
    "Построим своего агента, не написав ни строчки кода"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
